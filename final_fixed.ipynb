{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuroVocal AI: Vocal Biomarker Classifier (Training Pipeline)\n",
    "\n",
    "This notebook provides a complete, end-to-end pipeline for training the ensemble model used in the NeuroVocal AI web application. It trains two separate models directly from `.wav` files:\n",
    "\n",
    "1.  **CRNN Model**: A Convolutional Recurrent Neural Network trained on **Mel Spectrograms**.\n",
    "2.  **Random Forest Model**: A classic machine learning model trained on a **vector of summary statistics** (jitter, shimmer, MFCC means, etc.).\n",
    "\n",
    "This script automatically scans a directory of audio files, extracts both feature sets, trains the models, and saves all necessary components for the Flask backend (`app.py`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Configuration\n",
    "\n",
    "First, we import all the necessary libraries and set up the configuration for our project. **You only need to update the `DATA_DIR` variable to point to your main dataset folder.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import parselmouth\n",
    "from parselmouth.praat import call\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, LSTM, Dense, Dropout, BatchNormalization, Reshape\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Configuration ---\n",
    "class Config:\n",
    "    # --- USER ACTION REQUIRED: Update this path ---\n",
    "    DATA_DIR = r'C:\\Users\\yajna\\Downloads\\fiftydataset' # Main directory with subfolders for each class (e.g., 'healthy', 'parkinsons')\n",
    "\n",
    "    # Audio & Feature Parameters\n",
    "    SAMPLE_RATE = 44100\n",
    "    N_MELS_CRNN = 128\n",
    "    MAX_PAD_LEN_CRNN = 250 # Max length for spectrograms\n",
    "    N_MFCC_RF = 13 # Number of MFCCs for the Random Forest features\n",
    "\n",
    "    # Model & Training parameters\n",
    "    TEST_SIZE = 0.2\n",
    "    RANDOM_STATE = 42\n",
    "    N_ESTIMATORS_RF = 100\n",
    "    EPOCHS_CRNN = 50\n",
    "    BATCH_SIZE_CRNN = 32\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dual Feature Extraction\n",
    "\n",
    "We define two separate feature extraction functions, mirroring the `feature_extractor_advanced.py` file used by the Flask application.\n",
    "\n",
    "- `extract_features_for_rf`: Creates a single row of summary statistics for the Random Forest model.\n",
    "- `extract_spectrogram_for_crnn`: Creates a 2D Mel Spectrogram image for the CRNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_for_rf(wav_path):\n",
    "    \"\"\"Extracts a comprehensive feature vector (like a CSV row) for the Random Forest model.\"\"\"\n",
    "    features = {}\n",
    "    try:\n",
    "        y, sr = librosa.load(wav_path, sr=config.SAMPLE_RATE)\n",
    "        sound = parselmouth.Sound(wav_path)\n",
    "        \n",
    "        # Jitter & shimmer\n",
    "        point_process = call(sound, \"To PointProcess (periodic, cc)\", 75, 500)\n",
    "        features['jitter_local'] = call(point_process, \"Get jitter (local)\", 0, 0, 0.0001, 0.02, 1.3)\n",
    "        features['shimmer_local'] = call([sound, point_process], \"Get shimmer (local)\", 0, 0, 0.0001, 0.02, 1.3, 1.6)\n",
    "\n",
    "        # Harmonicity\n",
    "        harmonicity = call(sound, \"To Harmonicity (cc)\", 0.01, 75, 0.1, 1.0)\n",
    "        features['hnr'] = call(harmonicity, \"Get mean\", 0, 0)\n",
    "\n",
    "        # Pitch & intensity (fixed call!)\n",
    "        pitch = sound.to_pitch(None, 75, 600)   # <-- FIXED\n",
    "        intensity = sound.to_intensity()\n",
    "        features['mean_f0'] = call(pitch, \"Get mean\", 0, 0, \"Hertz\")\n",
    "        features['std_dev_f0'] = call(pitch, \"Get standard deviation\", 0, 0, \"Hertz\")\n",
    "        features['mean_intensity'] = call(intensity, \"Get mean\", 0, 0, \"energy\")\n",
    "\n",
    "        # MFCCs\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=config.N_MFCC_RF)\n",
    "        for i in range(config.N_MFCC_RF):\n",
    "            features[f'mfcc_{i+1}_mean'] = np.mean(mfccs[i])\n",
    "            features[f'mfcc_{i+1}_std'] = np.std(mfccs[i])\n",
    "\n",
    "        # Spectral features\n",
    "        features['spectral_centroid'] = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))\n",
    "        features['spectral_rolloff'] = np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr))\n",
    "        \n",
    "        # Handle NaN/inf\n",
    "        for key, value in features.items():\n",
    "            if np.isnan(value) or np.isinf(value):\n",
    "                features[key] = 0\n",
    "\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {os.path.basename(wav_path)} for RF features: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_spectrogram_for_crnn(wav_path):\n",
    "    \"\"\"Extracts a Mel Spectrogram for the CRNN model.\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(wav_path, sr=config.SAMPLE_RATE)\n",
    "        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=config.N_MELS_CRNN)\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        \n",
    "        # Pad or truncate spectrogram to a fixed length\n",
    "        if mel_spec_db.shape[1] > config.MAX_PAD_LEN_CRNN:\n",
    "            mel_spec_db = mel_spec_db[:, :config.MAX_PAD_LEN_CRNN]\n",
    "        else:\n",
    "            pad_width = config.MAX_PAD_LEN_CRNN - mel_spec_db.shape[1]\n",
    "            mel_spec_db = np.pad(mel_spec_db, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "        return mel_spec_db\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting spectrogram: {e}\")\n",
    "        return np.zeros((config.N_MELS_CRNN, config.MAX_PAD_LEN_CRNN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preprocessing\n",
    "\n",
    "This function now scans the subdirectories in `DATA_DIR`, using the folder names as labels. It extracts both sets of features for each `.wav` file it finds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data_from_folders():\n",
    "    print(\"Step 1: Loading data and extracting features from audio folders...\")\n",
    "    \n",
    "    if not os.path.exists(config.DATA_DIR):\n",
    "        print(f\"\\n--- ERROR: Data directory '{config.DATA_DIR}' not found. ---\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    rf_features_list = []\n",
    "    crnn_features_list = []\n",
    "    labels = []\n",
    "\n",
    "    # Iterate through each subfolder (which represents a class/label)\n",
    "    for label in sorted(os.listdir(config.DATA_DIR)):\n",
    "        class_dir = os.path.join(config.DATA_DIR, label)\n",
    "        if os.path.isdir(class_dir):\n",
    "            print(f\"Processing files for class: {label}\")\n",
    "            for filename in os.listdir(class_dir):\n",
    "                if filename.endswith('.wav'):\n",
    "                    file_path = os.path.join(class_dir, filename)\n",
    "                    \n",
    "                    # Extract both sets of features\n",
    "                    rf_features = extract_features_for_rf(file_path)\n",
    "                    crnn_features = extract_spectrogram_for_crnn(file_path)\n",
    "                    \n",
    "                    if rf_features is not None and crnn_features is not None:\n",
    "                        rf_features_list.append(rf_features)\n",
    "                        crnn_features_list.append(crnn_features)\n",
    "                        labels.append(label)\n",
    "    \n",
    "    if not rf_features_list:\n",
    "        print(\"\\n--- ERROR: No .wav files were found and processed. Check your DATA_DIR path and folder structure. ---\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    df_rf = pd.DataFrame(rf_features_list)\n",
    "    X_crnn = np.array(crnn_features_list)\n",
    "    \n",
    "    print(f\"\\nSuccessfully loaded and processed {len(df_rf)} files.\")\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(labels)\n",
    "    \n",
    "    return df_rf, X_crnn, y_encoded, label_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Architecture (Deeper)\n",
    "\n",
    "This function defines the **deeper** CRNN model architecture. We have added an extra `Conv2D` block and an extra `Dense` layer to increase its learning capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_crnn_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        # Ensure input is explicitly defined for CRNN\n",
    "        Reshape((input_shape[0], input_shape[1], 1), input_shape=input_shape),\n",
    "\n",
    "        # Block 1\n",
    "        tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', padding=\"same\"),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "        # Block 2\n",
    "        tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu', padding=\"same\"),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "        # Block 3\n",
    "        tf.keras.layers.Conv2D(128, kernel_size=(3, 3), activation='relu', padding=\"same\"),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "        # Flatten CNN output for RNN\n",
    "        tf.keras.layers.Reshape((-1, 128)),\n",
    "\n",
    "        # RNN layers\n",
    "        tf.keras.layers.GRU(128, return_sequences=True),\n",
    "        tf.keras.layers.GRU(64),\n",
    "\n",
    "        # Dense layers\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "\n",
    "        # Output layer\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Main Training and Evaluation Pipeline\n",
    "\n",
    "This is the main execution block. It runs the entire pipeline step-by-step:\n",
    "1. Loads the data directly from audio folders.\n",
    "2. Splits data into training and testing sets.\n",
    "3. **Scales the Random Forest features and applies SMOTE** to the training set to handle class imbalance.\n",
    "4. Trains the Random Forest model.\n",
    "5. Trains the CRNN model on the spectrograms.\n",
    "6. Combines the predictions from both models (ensemble) and evaluates the final performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting NeuroVocal AI Training Pipeline ---\n",
      "Step 1: Loading data and extracting features from audio folders...\n",
      "Processing files for class: alzheimer\n",
      "Processing files for class: depression\n",
      "Processing files for class: healthy\n",
      "Processing files for class: parkinson\n",
      "Error processing PD1a1_LF - Copy.wav for RF features: To analyse this Sound, “minimum pitch” must not be less than 324.3243243243243 Hz.\n",
      "Sound \"untitled\": pitch analysis not performed.\n",
      "Sound \"untitled\": periodic pulses (cc) not computed.\n",
      "Error processing PD1a1_LF.wav for RF features: To analyse this Sound, “minimum pitch” must not be less than 324.3243243243243 Hz.\n",
      "Sound \"untitled\": pitch analysis not performed.\n",
      "Sound \"untitled\": periodic pulses (cc) not computed.\n",
      "Error processing PD1a2_LF - Copy.wav for RF features: To analyse this Sound, “minimum pitch” must not be less than 313.04347826086956 Hz.\n",
      "Sound \"untitled\": pitch analysis not performed.\n",
      "Sound \"untitled\": periodic pulses (cc) not computed.\n",
      "Error processing PD1a2_LF.wav for RF features: To analyse this Sound, “minimum pitch” must not be less than 313.04347826086956 Hz.\n",
      "Sound \"untitled\": pitch analysis not performed.\n",
      "Sound \"untitled\": periodic pulses (cc) not computed.\n",
      "Error processing PD1i1_LF - Copy.wav for RF features: To analyse this Sound, “minimum pitch” must not be less than 338.02816901408454 Hz.\n",
      "Sound \"untitled\": pitch analysis not performed.\n",
      "Sound \"untitled\": periodic pulses (cc) not computed.\n",
      "Error processing PD1i1_LF.wav for RF features: To analyse this Sound, “minimum pitch” must not be less than 338.02816901408454 Hz.\n",
      "Sound \"untitled\": pitch analysis not performed.\n",
      "Sound \"untitled\": periodic pulses (cc) not computed.\n",
      "Error processing PD1i2_LF - Copy.wav for RF features: To analyse this Sound, “minimum pitch” must not be less than 338.8235294117647 Hz.\n",
      "Sound \"untitled\": pitch analysis not performed.\n",
      "Sound \"untitled\": periodic pulses (cc) not computed.\n",
      "Error processing PD1i2_LF.wav for RF features: To analyse this Sound, “minimum pitch” must not be less than 338.8235294117647 Hz.\n",
      "Sound \"untitled\": pitch analysis not performed.\n",
      "Sound \"untitled\": periodic pulses (cc) not computed.\n",
      "Error processing PD2a1_LF - Copy.wav for RF features: To analyse this Sound, “minimum pitch” must not be less than 404.49438202247194 Hz.\n",
      "Sound \"untitled\": pitch analysis not performed.\n",
      "Sound \"untitled\": periodic pulses (cc) not computed.\n",
      "Error processing PD2a1_LF.wav for RF features: To analyse this Sound, “minimum pitch” must not be less than 404.49438202247194 Hz.\n",
      "Sound \"untitled\": pitch analysis not performed.\n",
      "Sound \"untitled\": periodic pulses (cc) not computed.\n",
      "Error processing PD2a2_LF - Copy.wav for RF features: To analyse this Sound, “minimum pitch” must not be less than 404.49438202247194 Hz.\n",
      "Sound \"untitled\": pitch analysis not performed.\n",
      "Sound \"untitled\": periodic pulses (cc) not computed.\n",
      "Error processing PD2a2_LF.wav for RF features: To analyse this Sound, “minimum pitch” must not be less than 404.49438202247194 Hz.\n",
      "Sound \"untitled\": pitch analysis not performed.\n",
      "Sound \"untitled\": periodic pulses (cc) not computed.\n",
      "Error processing PD2i1_LF - Copy.wav for RF features: To analyse this Sound, “minimum pitch” must not be less than 405.63380281690144 Hz.\n",
      "Sound \"untitled\": pitch analysis not performed.\n",
      "Sound \"untitled\": periodic pulses (cc) not computed.\n",
      "Error processing PD2i1_LF.wav for RF features: To analyse this Sound, “minimum pitch” must not be less than 405.63380281690144 Hz.\n",
      "Sound \"untitled\": pitch analysis not performed.\n",
      "Sound \"untitled\": periodic pulses (cc) not computed.\n",
      "Error processing PD2i2_LF - Copy.wav for RF features: To analyse this Sound, “minimum pitch” must not be less than 409.0909090909091 Hz.\n",
      "Sound \"untitled\": pitch analysis not performed.\n",
      "Sound \"untitled\": periodic pulses (cc) not computed.\n",
      "Error processing PD2i2_LF.wav for RF features: To analyse this Sound, “minimum pitch” must not be less than 409.0909090909091 Hz.\n",
      "Sound \"untitled\": pitch analysis not performed.\n",
      "Sound \"untitled\": periodic pulses (cc) not computed.\n",
      "Error processing PD3a1_LF.wav for RF features: To analyse this Sound, “minimum pitch” must not be less than 283.46456692913387 Hz.\n",
      "Sound \"untitled\": pitch analysis not performed.\n",
      "Sound \"untitled\": periodic pulses (cc) not computed.\n",
      "Error processing PD3a2_LF.wav for RF features: To analyse this Sound, “minimum pitch” must not be less than 296.2962962962963 Hz.\n",
      "Sound \"untitled\": pitch analysis not performed.\n",
      "Sound \"untitled\": periodic pulses (cc) not computed.\n",
      "Error processing PD3a3_LF.wav for RF features: To analyse this Sound, “minimum pitch” must not be less than 296.9072164948454 Hz.\n",
      "Sound \"untitled\": pitch analysis not performed.\n",
      "Sound \"untitled\": periodic pulses (cc) not computed.\n",
      "Error processing PD3i1_LF.wav for RF features: To analyse this Sound, “minimum pitch” must not be less than 325.0564334085779 Hz.\n",
      "Sound \"untitled\": pitch analysis not performed.\n",
      "Sound \"untitled\": periodic pulses (cc) not computed.\n",
      "Error processing PD3i2_LF.wav for RF features: To analyse this Sound, “minimum pitch” must not be less than 299.3762993762994 Hz.\n",
      "Sound \"untitled\": pitch analysis not performed.\n",
      "Sound \"untitled\": periodic pulses (cc) not computed.\n",
      "Error processing PD4a1_LF.wav for RF features: To analyse this Sound, “minimum pitch” must not be less than 496.55172413793105 Hz.\n",
      "Sound \"untitled\": pitch analysis not performed.\n",
      "Sound \"untitled\": periodic pulses (cc) not computed.\n",
      "Error processing PD4a2_LF.wav for RF features: To analyse this Sound, “minimum pitch” must not be less than 476.8211920529801 Hz.\n",
      "Sound \"untitled\": pitch analysis not performed.\n",
      "Sound \"untitled\": periodic pulses (cc) not computed.\n",
      "Error processing PD4i1_LF.wav for RF features: To analyse this Sound, “minimum pitch” must not be less than 491.4675767918089 Hz.\n",
      "Sound \"untitled\": pitch analysis not performed.\n",
      "Sound \"untitled\": periodic pulses (cc) not computed.\n",
      "Error processing PD4i2_LF.wav for RF features: To analyse this Sound, “minimum pitch” must not be less than 493.1506849315069 Hz.\n",
      "Sound \"untitled\": pitch analysis not performed.\n",
      "Sound \"untitled\": periodic pulses (cc) not computed.\n",
      "Error processing PD5a1_LF.wav for RF features: To analyse this Sound, “minimum pitch” must not be less than 406.77966101694915 Hz.\n",
      "Sound \"untitled\": pitch analysis not performed.\n",
      "Sound \"untitled\": periodic pulses (cc) not computed.\n",
      "Error processing PD5a2_LF.wav for RF features: To analyse this Sound, “minimum pitch” must not be less than 480.00000000000006 Hz.\n",
      "Sound \"untitled\": pitch analysis not performed.\n",
      "Sound \"untitled\": periodic pulses (cc) not computed.\n",
      "Error processing PD5a3_LF.wav for RF features: To analyse this Sound, “minimum pitch” must not be less than 475.2475247524753 Hz.\n",
      "Sound \"untitled\": pitch analysis not performed.\n",
      "Sound \"untitled\": periodic pulses (cc) not computed.\n",
      "Error processing PD5i1_LF.wav for RF features: To analyse this Sound, “minimum pitch” must not be less than 466.0194174757282 Hz.\n",
      "Sound \"untitled\": pitch analysis not performed.\n",
      "Sound \"untitled\": periodic pulses (cc) not computed.\n",
      "Error processing PD5i2_LF.wav for RF features: To analyse this Sound, “minimum pitch” must not be less than 481.60535117056855 Hz.\n",
      "Sound \"untitled\": pitch analysis not performed.\n",
      "Sound \"untitled\": periodic pulses (cc) not computed.\n",
      "Error processing PD6a1_LF.wav for RF features: To analyse this Sound, “minimum pitch” must not be less than 620.6896551724137 Hz.\n",
      "Sound \"untitled\": pitch analysis not performed.\n",
      "Sound \"untitled\": periodic pulses (cc) not computed.\n",
      "Error processing PD6a2_LF.wav for RF features: To analyse this Sound, “minimum pitch” must not be less than 628.82096069869 Hz.\n",
      "Sound \"untitled\": pitch analysis not performed.\n",
      "Sound \"untitled\": periodic pulses (cc) not computed.\n",
      "Error processing PD6i1_LF.wav for RF features: To analyse this Sound, “minimum pitch” must not be less than 657.5342465753425 Hz.\n",
      "Sound \"untitled\": pitch analysis not performed.\n",
      "Sound \"untitled\": periodic pulses (cc) not computed.\n",
      "Error processing PD6i2_LF.wav for RF features: To analyse this Sound, “minimum pitch” must not be less than 695.6521739130436 Hz.\n",
      "Sound \"untitled\": pitch analysis not performed.\n",
      "Sound \"untitled\": periodic pulses (cc) not computed.\n",
      "Error processing PD7a1_LF.wav for RF features: To analyse this Sound, “minimum pitch” must not be less than 361.8090452261307 Hz.\n",
      "Sound \"untitled\": pitch analysis not performed.\n",
      "Sound \"untitled\": periodic pulses (cc) not computed.\n",
      "Error processing PD7a2_LF.wav for RF features: To analyse this Sound, “minimum pitch” must not be less than 362.72040302267004 Hz.\n",
      "Sound \"untitled\": pitch analysis not performed.\n",
      "Sound \"untitled\": periodic pulses (cc) not computed.\n",
      "Error processing PD7i1_LF.wav for RF features: To analyse this Sound, “minimum pitch” must not be less than 371.1340206185567 Hz.\n",
      "Sound \"untitled\": pitch analysis not performed.\n",
      "Sound \"untitled\": periodic pulses (cc) not computed.\n",
      "Error processing PD7i2_LF.wav for RF features: To analyse this Sound, “minimum pitch” must not be less than 377.9527559055118 Hz.\n",
      "Sound \"untitled\": pitch analysis not performed.\n",
      "Sound \"untitled\": periodic pulses (cc) not computed.\n",
      "Error processing PD8a1_LF.wav for RF features: To analyse this Sound, “minimum pitch” must not be less than 709.35960591133 Hz.\n",
      "Sound \"untitled\": pitch analysis not performed.\n",
      "Sound \"untitled\": periodic pulses (cc) not computed.\n",
      "Error processing PD8a2_LF.wav for RF features: To analyse this Sound, “minimum pitch” must not be less than 702.439024390244 Hz.\n",
      "Sound \"untitled\": pitch analysis not performed.\n",
      "Sound \"untitled\": periodic pulses (cc) not computed.\n",
      "Error processing PD8i1_LF.wav for RF features: To analyse this Sound, “minimum pitch” must not be less than 716.4179104477612 Hz.\n",
      "Sound \"untitled\": pitch analysis not performed.\n",
      "Sound \"untitled\": periodic pulses (cc) not computed.\n",
      "Error processing PD8i2_LF.wav for RF features: To analyse this Sound, “minimum pitch” must not be less than 712.8712871287129 Hz.\n",
      "Sound \"untitled\": pitch analysis not performed.\n",
      "Sound \"untitled\": periodic pulses (cc) not computed.\n",
      "Error processing PD9a1_LF.wav for RF features: To analyse this Sound, “minimum pitch” must not be less than 413.7931034482759 Hz.\n",
      "Sound \"untitled\": pitch analysis not performed.\n",
      "Sound \"untitled\": periodic pulses (cc) not computed.\n",
      "Processing files for class: stress\n",
      "Processing files for class: working models\n",
      "\n",
      "Successfully loaded and processed 942 files.\n",
      "\n",
      "Step 2: Splitting data...\n",
      "Step 3: Scaling RF features and applying SMOTE...\n",
      "\n",
      "Step 4: Training Random Forest model...\n",
      "\n",
      "Step 5: Training CRNN model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">125</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">125</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">125</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">496</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">496</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">99,072</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">37,248</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">325</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ reshape (\u001b[38;5;33mReshape\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m250\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m250\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m125\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m125\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m125\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape_1 (\u001b[38;5;33mReshape\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m496\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru (\u001b[38;5;33mGRU\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m496\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m99,072\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_1 (\u001b[38;5;33mGRU\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m37,248\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │           \u001b[38;5;34m325\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">234,373</span> (915.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m234,373\u001b[0m (915.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">233,925</span> (913.77 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m233,925\u001b[0m (913.77 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 1s/step - accuracy: 0.4382 - loss: 1.2402 - val_accuracy: 0.4233 - val_loss: 1.5234\n",
      "Epoch 2/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 572ms/step - accuracy: 0.6401 - loss: 0.8651 - val_accuracy: 0.4180 - val_loss: 2.2968\n",
      "Epoch 3/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 631ms/step - accuracy: 0.7291 - loss: 0.6523 - val_accuracy: 0.4127 - val_loss: 2.6918\n",
      "Epoch 4/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 894ms/step - accuracy: 0.8433 - loss: 0.4595 - val_accuracy: 0.4233 - val_loss: 2.4441\n",
      "Epoch 5/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 544ms/step - accuracy: 0.9044 - loss: 0.3185 - val_accuracy: 0.5661 - val_loss: 1.5090\n",
      "Epoch 6/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 525ms/step - accuracy: 0.9031 - loss: 0.2945 - val_accuracy: 0.6085 - val_loss: 1.1258\n",
      "Epoch 7/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 511ms/step - accuracy: 0.9137 - loss: 0.2235 - val_accuracy: 0.5344 - val_loss: 1.4012\n",
      "Epoch 8/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 511ms/step - accuracy: 0.9416 - loss: 0.1885 - val_accuracy: 0.4392 - val_loss: 2.5582\n",
      "Epoch 9/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 518ms/step - accuracy: 0.9641 - loss: 0.1428 - val_accuracy: 0.7778 - val_loss: 0.5926\n",
      "Epoch 10/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 868ms/step - accuracy: 0.9668 - loss: 0.1177 - val_accuracy: 0.9259 - val_loss: 0.2155\n",
      "Epoch 11/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 598ms/step - accuracy: 0.9748 - loss: 0.0846 - val_accuracy: 0.9630 - val_loss: 0.1257\n",
      "Epoch 12/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 613ms/step - accuracy: 0.9748 - loss: 0.0956 - val_accuracy: 0.9471 - val_loss: 0.1669\n",
      "Epoch 13/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 607ms/step - accuracy: 0.9841 - loss: 0.0555 - val_accuracy: 0.8677 - val_loss: 0.4329\n",
      "Epoch 14/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 610ms/step - accuracy: 0.9801 - loss: 0.0693 - val_accuracy: 0.7407 - val_loss: 0.8447\n",
      "Epoch 15/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 637ms/step - accuracy: 0.9788 - loss: 0.0707 - val_accuracy: 0.7249 - val_loss: 1.3633\n",
      "Epoch 16/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 690ms/step - accuracy: 0.9934 - loss: 0.0304 - val_accuracy: 0.6508 - val_loss: 1.8404\n",
      "Epoch 17/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 623ms/step - accuracy: 0.9987 - loss: 0.0102 - val_accuracy: 0.6614 - val_loss: 1.9203\n",
      "Epoch 18/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 620ms/step - accuracy: 0.9894 - loss: 0.0421 - val_accuracy: 0.9894 - val_loss: 0.0463\n",
      "Epoch 19/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 624ms/step - accuracy: 0.9973 - loss: 0.0193 - val_accuracy: 0.8519 - val_loss: 0.3852\n",
      "Epoch 20/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 633ms/step - accuracy: 0.9960 - loss: 0.0160 - val_accuracy: 0.8677 - val_loss: 0.6637\n",
      "Epoch 21/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 621ms/step - accuracy: 0.9960 - loss: 0.0111 - val_accuracy: 0.9206 - val_loss: 0.2398\n",
      "Epoch 22/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 632ms/step - accuracy: 1.0000 - loss: 0.0044 - val_accuracy: 0.9894 - val_loss: 0.0664\n",
      "Epoch 23/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 636ms/step - accuracy: 1.0000 - loss: 0.0029 - val_accuracy: 0.9947 - val_loss: 0.0471\n",
      "Epoch 24/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 567ms/step - accuracy: 1.0000 - loss: 0.0030 - val_accuracy: 0.9894 - val_loss: 0.0484\n",
      "Epoch 25/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 535ms/step - accuracy: 1.0000 - loss: 0.0026 - val_accuracy: 0.9947 - val_loss: 0.0485\n",
      "Epoch 26/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 526ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.9947 - val_loss: 0.0487\n",
      "Epoch 27/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 528ms/step - accuracy: 1.0000 - loss: 0.0019 - val_accuracy: 0.9947 - val_loss: 0.0507\n",
      "Epoch 28/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 561ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.9947 - val_loss: 0.0522\n",
      "Epoch 29/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 584ms/step - accuracy: 1.0000 - loss: 0.0017 - val_accuracy: 0.9947 - val_loss: 0.0521\n",
      "Epoch 30/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 535ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.9947 - val_loss: 0.0529\n",
      "Epoch 31/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 537ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.9947 - val_loss: 0.0544\n",
      "Epoch 32/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 528ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.9894 - val_loss: 0.0560\n",
      "Epoch 33/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 545ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.9894 - val_loss: 0.0593\n",
      "Epoch 34/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 562ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 0.9894 - val_loss: 0.0615\n",
      "Epoch 35/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 881ms/step - accuracy: 1.0000 - loss: 8.8650e-04 - val_accuracy: 0.9894 - val_loss: 0.0595\n",
      "Epoch 36/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 734ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.9894 - val_loss: 0.0583\n",
      "Epoch 37/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 521ms/step - accuracy: 1.0000 - loss: 9.1619e-04 - val_accuracy: 0.9947 - val_loss: 0.0570\n",
      "Epoch 38/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 564ms/step - accuracy: 1.0000 - loss: 7.8354e-04 - val_accuracy: 0.9947 - val_loss: 0.0571\n",
      "Epoch 39/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 983ms/step - accuracy: 1.0000 - loss: 8.1218e-04 - val_accuracy: 0.9947 - val_loss: 0.0577\n",
      "Epoch 40/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 989ms/step - accuracy: 1.0000 - loss: 5.2287e-04 - val_accuracy: 0.9947 - val_loss: 0.0584\n",
      "Epoch 41/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 972ms/step - accuracy: 1.0000 - loss: 8.2003e-04 - val_accuracy: 0.9894 - val_loss: 0.0606\n",
      "Epoch 42/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 962ms/step - accuracy: 1.0000 - loss: 7.4102e-04 - val_accuracy: 0.9894 - val_loss: 0.0612\n",
      "Epoch 43/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 959ms/step - accuracy: 1.0000 - loss: 6.8698e-04 - val_accuracy: 0.9894 - val_loss: 0.0621\n",
      "Epoch 44/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 959ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.9894 - val_loss: 0.0619\n",
      "Epoch 45/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 951ms/step - accuracy: 1.0000 - loss: 5.1535e-04 - val_accuracy: 0.9894 - val_loss: 0.0624\n",
      "Epoch 46/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 951ms/step - accuracy: 1.0000 - loss: 5.0632e-04 - val_accuracy: 0.9894 - val_loss: 0.0628\n",
      "Epoch 47/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 956ms/step - accuracy: 1.0000 - loss: 5.3605e-04 - val_accuracy: 0.9894 - val_loss: 0.0641\n",
      "Epoch 48/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 954ms/step - accuracy: 1.0000 - loss: 4.3368e-04 - val_accuracy: 0.9894 - val_loss: 0.0643\n",
      "Epoch 49/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 981ms/step - accuracy: 1.0000 - loss: 4.2621e-04 - val_accuracy: 0.9894 - val_loss: 0.0643\n",
      "Epoch 50/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 986ms/step - accuracy: 1.0000 - loss: 4.7832e-04 - val_accuracy: 0.9894 - val_loss: 0.0651\n",
      "\n",
      "Step 6: Evaluating the ensemble model...\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 335ms/step\n",
      "\n",
      "--- Final Ensemble Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   alzheimer       1.00      0.98      0.99        41\n",
      "  depression       1.00      1.00      1.00        41\n",
      "     healthy       1.00      1.00      1.00        41\n",
      "   parkinson       0.96      1.00      0.98        25\n",
      "      stress       1.00      1.00      1.00        41\n",
      "\n",
      "    accuracy                           0.99       189\n",
      "   macro avg       0.99      1.00      0.99       189\n",
      "weighted avg       0.99      0.99      0.99       189\n",
      "\n",
      "Final Ensemble Accuracy: 99.47%\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Starting NeuroVocal AI Training Pipeline ---\")\n",
    "\n",
    "# Step 1: Load data\n",
    "X_rf_df, X_crnn, y, label_encoder = load_and_prepare_data_from_folders()\n",
    "\n",
    "if X_rf_df is not None:\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    X_rf = X_rf_df.values\n",
    "\n",
    "    # Step 2: Split data\n",
    "    print(\"\\nStep 2: Splitting data...\")\n",
    "    X_rf_train, X_rf_test, X_crnn_train, X_crnn_test, y_train, y_test = train_test_split(\n",
    "        X_rf, X_crnn, y, test_size=config.TEST_SIZE, random_state=config.RANDOM_STATE, stratify=y\n",
    "    )\n",
    "\n",
    "    # Step 3: Scale RF features and apply SMOTE\n",
    "    print(\"Step 3: Scaling RF features and applying SMOTE...\")\n",
    "    scaler_rf = StandardScaler()\n",
    "    X_rf_train_scaled = scaler_rf.fit_transform(X_rf_train)\n",
    "    X_rf_test_scaled = scaler_rf.transform(X_rf_test)\n",
    "    \n",
    "    smote = SMOTE(random_state=config.RANDOM_STATE)\n",
    "    X_rf_train_resampled, y_train_resampled = smote.fit_resample(X_rf_train_scaled, y_train)\n",
    "\n",
    "    # Step 4: Train Random Forest Model\n",
    "    print(\"\\nStep 4: Training Random Forest model...\")\n",
    "    rf_model = RandomForestClassifier(n_estimators=config.N_ESTIMATORS_RF, random_state=config.RANDOM_STATE)\n",
    "    rf_model.fit(X_rf_train_resampled, y_train_resampled)\n",
    "\n",
    "    # Step 5: Train CRNN Model\n",
    "    print(\"\\nStep 5: Training CRNN model...\")\n",
    "    # We use the original y_train for CRNN to match X_crnn_train\n",
    "    y_train_categorical = to_categorical(y_train, num_classes=num_classes)\n",
    "    y_test_categorical = to_categorical(y_test, num_classes=num_classes)\n",
    "    \n",
    "    crnn_model = create_crnn_model(input_shape=(config.N_MELS_CRNN, config.MAX_PAD_LEN_CRNN), num_classes=num_classes)\n",
    "    crnn_model.summary() # Print model details\n",
    "    crnn_model.fit(X_crnn_train, y_train_categorical, \n",
    "                   epochs=config.EPOCHS_CRNN, batch_size=config.BATCH_SIZE_CRNN, \n",
    "                   validation_data=(X_crnn_test, y_test_categorical), verbose=1)\n",
    "\n",
    "    # Step 6: Ensemble Evaluation\n",
    "    print(\"\\nStep 6: Evaluating the ensemble model...\")\n",
    "    rf_probs = rf_model.predict_proba(X_rf_test_scaled)\n",
    "    crnn_probs = crnn_model.predict(X_crnn_test)\n",
    "    \n",
    "    ensemble_probs = (crnn_probs + rf_probs) / 2.0\n",
    "    ensemble_preds = np.argmax(ensemble_probs, axis=1)\n",
    "    \n",
    "    print(\"\\n--- Final Ensemble Classification Report ---\")\n",
    "    print(classification_report(y_test, ensemble_preds, target_names=label_encoder.classes_))\n",
    "    print(f\"Final Ensemble Accuracy: {accuracy_score(y_test, ensemble_preds) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Models and Objects for Deployment\n",
    "\n",
    "This final step saves all the components needed by the `app.py` backend server. After running this cell, you will have the following files ready for your web application:\n",
    "- `crnn_model.h5`: The trained Keras CRNN model.\n",
    "- `random_forest_model.joblib`: The trained Scikit-learn Random Forest model.\n",
    "- `audio_scaler.joblib`: The scaler fitted on the Random Forest training data.\n",
    "- `label_encoder.joblib`: The label encoder to convert model outputs back to class names.\n",
    "- `rf_feature_columns.joblib`: A list of the feature names in the correct order for the RF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Saving all components for deployment ---\n",
      "Saved crnn_model.h5\n",
      "Saved random_forest_model.joblib\n",
      "Saved audio_scaler.joblib\n",
      "Saved label_encoder.joblib\n",
      "Saved rf_feature_columns.joblib\n",
      "\n",
      "All components are ready for the Flask application!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Saving all components for deployment ---\")\n",
    "\n",
    "# Save the CRNN model\n",
    "crnn_model.save('crnn_model.h5')\n",
    "print(\"Saved crnn_model.h5\")\n",
    "\n",
    "# Save the Random Forest model\n",
    "joblib.dump(rf_model, 'random_forest_model.joblib')\n",
    "print(\"Saved random_forest_model.joblib\")cd\n",
    "\n",
    "# Save the scaler for the RF features\n",
    "joblib.dump(scaler_rf, 'audio_scaler.joblib')\n",
    "print(\"Saved audio_scaler.joblib\")\n",
    "\n",
    "# Save the label encoder\n",
    "joblib.dump(label_encoder, 'label_encoder.joblib')\n",
    "print(\"Saved label_encoder.joblib\")\n",
    "\n",
    "# Save the column order for the RF model\n",
    "joblib.dump(list(X_rf_df.columns), 'rf_feature_columns.joblib')\n",
    "print(\"Saved rf_feature_columns.joblib\")\n",
    "\n",
    "print(\"\\nAll components are ready for the Flask application!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved models and assets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All components loaded successfully!\n",
      "\n",
      "▶️  Analyzing file: 'recall_705-0_processed.wav'...\n",
      "\n",
      "===================================\n",
      "      ✅ FINAL PREDICTION\n",
      "===================================\n",
      "The predicted class is: ALZHIMER\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import parselmouth\n",
    "from parselmouth.praat import call\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "\n",
    "# Suppress unnecessary warnings for a cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "\n",
    "# --- 1. Configuration (Must be identical to the training script) ---\n",
    "class Config:\n",
    "    SAMPLE_RATE = 44100\n",
    "    N_MELS_CRNN = 128\n",
    "    MAX_PAD_LEN_CRNN = 250\n",
    "    N_MFCC_RF = 13\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# --- 2. Load All Saved Models and Assets ---\n",
    "# This block simulates the startup of your web application\n",
    "try:\n",
    "    print(\"Loading saved models and assets...\")\n",
    "    CRNN_MODEL = tf.keras.models.load_model('crnn_model.h5')\n",
    "    RF_MODEL = joblib.load('random_forest_model.joblib')\n",
    "    SCALER = joblib.load('audio_scaler.joblib')\n",
    "    LABEL_ENCODER = joblib.load('label_encoder.joblib')\n",
    "    RF_FEATURE_COLUMNS = joblib.load('rf_feature_columns.joblib')\n",
    "    print(\"✅ All components loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading files: {e}\")\n",
    "    print(\"Please ensure all .h5 and .joblib files are in the same directory as the notebook.\")\n",
    "\n",
    "# --- 3. Feature Extraction Functions (Copied from your notebook) ---\n",
    "def extract_features_for_rf(wav_path):\n",
    "    features = {}\n",
    "    try:\n",
    "        y, sr = librosa.load(wav_path, sr=config.SAMPLE_RATE)\n",
    "        sound = parselmouth.Sound(wav_path)\n",
    "        point_process = call(sound, \"To PointProcess (periodic, cc)\", 75, 500)\n",
    "        features['jitter_local'] = call(point_process, \"Get jitter (local)\", 0, 0, 0.0001, 0.02, 1.3)\n",
    "        features['shimmer_local'] = call([sound, point_process], \"Get shimmer (local)\", 0, 0, 0.0001, 0.02, 1.3, 1.6)\n",
    "        harmonicity = call(sound, \"To Harmonicity (cc)\", 0.01, 75, 0.1, 1.0)\n",
    "        features['hnr'] = call(harmonicity, \"Get mean\", 0, 0)\n",
    "        pitch = sound.to_pitch(None, 75, 600)\n",
    "        intensity = sound.to_intensity()\n",
    "        features['mean_f0'] = call(pitch, \"Get mean\", 0, 0, \"Hertz\")\n",
    "        features['std_dev_f0'] = call(pitch, \"Get standard deviation\", 0, 0, \"Hertz\")\n",
    "        features['mean_intensity'] = call(intensity, \"Get mean\", 0, 0, \"energy\")\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=config.N_MFCC_RF)\n",
    "        for i in range(config.N_MFCC_RF):\n",
    "            features[f'mfcc_{i+1}_mean'] = np.mean(mfccs[i])\n",
    "            features[f'mfcc_{i+1}_std'] = np.std(mfccs[i])\n",
    "        features['spectral_centroid'] = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))\n",
    "        features['spectral_rolloff'] = np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr))\n",
    "        for key, value in features.items():\n",
    "            if np.isnan(value) or np.isinf(value):\n",
    "                features[key] = 0\n",
    "        return features\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def extract_spectrogram_for_crnn(wav_path):\n",
    "    try:\n",
    "        y, sr = librosa.load(wav_path, sr=config.SAMPLE_RATE)\n",
    "        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=config.N_MELS_CRNN)\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        if mel_spec_db.shape[1] > config.MAX_PAD_LEN_CRNN:\n",
    "            mel_spec_db = mel_spec_db[:, :config.MAX_PAD_LEN_CRNN]\n",
    "        else:\n",
    "            pad_width = config.MAX_PAD_LEN_CRNN - mel_spec_db.shape[1]\n",
    "            mel_spec_db = np.pad(mel_spec_db, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "        return mel_spec_db\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# --- 4. Main Prediction Logic ---\n",
    "\n",
    "# --- ACTION REQUIRED: Update the path to the audio file you want to test ---\n",
    "file_to_predict = r'C:\\\\Users\\\\yajna\\\\Downloads\\\\preprocessing\\\\processed_audio_wav_alzimers_recall\\\\recall_705-0_processed.wav'\n",
    "\n",
    "if not os.path.exists(file_to_predict):\n",
    "    print(f\"❌ Error: The file was not found. Please check the path: {file_to_predict}\")\n",
    "else:\n",
    "    try:\n",
    "        print(f\"\\n▶️  Analyzing file: '{os.path.basename(file_to_predict)}'...\")\n",
    "        \n",
    "        # Step 1: Extract both sets of features\n",
    "        rf_features = extract_features_for_rf(file_to_predict)\n",
    "        crnn_spec = extract_spectrogram_for_crnn(file_to_predict)\n",
    "        \n",
    "        if rf_features is not None and crnn_spec is not None:\n",
    "            # Step 2: Preprocess features for Random Forest\n",
    "            rf_df = pd.DataFrame([rf_features], columns=RF_FEATURE_COLUMNS)\n",
    "            rf_scaled = SCALER.transform(rf_df)\n",
    "            \n",
    "            # Step 3: Preprocess features for CRNN\n",
    "            crnn_reshaped = np.expand_dims(crnn_spec, axis=0)\n",
    "            \n",
    "            # Step 4: Get predictions from the loaded models\n",
    "            rf_probs = RF_MODEL.predict_proba(rf_scaled)\n",
    "            crnn_probs = CRNN_MODEL.predict(crnn_reshaped, verbose=0)\n",
    "            \n",
    "            # Step 5: Ensemble the predictions\n",
    "            ensemble_probs = (rf_probs + crnn_probs) / 2.0\n",
    "            prediction_index = np.argmax(ensemble_probs, axis=1)\n",
    "            \n",
    "            # Step 6: Decode the result using the loaded encoder\n",
    "            prediction_label = LABEL_ENCODER.inverse_transform(prediction_index)\n",
    "            \n",
    "            # Step 7: Display the final result\n",
    "            print(\"\\n\" + \"=\"*35)\n",
    "            print(\"      ✅ FINAL PREDICTION\")\n",
    "            print(\"=\"*35)\n",
    "            print(f\"The predicted class is: {prediction_label[0].upper()}\")\n",
    "            print(\"=\"*35)\n",
    "        else:\n",
    "            print(\"❌ Error: Feature extraction failed. The audio file might be too short or corrupted.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ An unexpected error occurred during prediction: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
